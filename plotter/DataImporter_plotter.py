"""
DataImporter_plotter.py

ã€åŠŸèƒ½èªªæ˜ã€‘
------------------------------------------------------------
æœ¬æ¨¡çµ„ç‚º Lo2cin4BT å¯è¦–åŒ–å¹³å°çš„æ•¸æ“šå°å…¥æ ¸å¿ƒæ¨¡çµ„ï¼Œè² è²¬è®€å–å’Œè§£æ metricstracker ç”¢ç”Ÿçš„ parquet æª”æ¡ˆï¼Œæ”¯æ´æƒææŒ‡å®šè³‡æ–™å¤¾ã€è§£æåƒæ•¸çµ„åˆã€æå–ç¸¾æ•ˆæŒ‡æ¨™å’Œæ¬Šç›Šæ›²ç·šæ•¸æ“šã€‚

ã€æµç¨‹èˆ‡æ•¸æ“šæµã€‘
------------------------------------------------------------
- ä¸»æµç¨‹ï¼šæƒæç›®éŒ„ â†’ è®€å–æª”æ¡ˆ â†’ è§£æåƒæ•¸ â†’ æå–æ•¸æ“š â†’ è¿”å›çµæœ
- å„æ¨¡çµ„é–“æ•¸æ“šæµæ˜ç¢ºï¼Œæµç¨‹å¦‚ä¸‹ï¼š

```mermaid
flowchart TD
    A[DataImporterPlotter] -->|æƒæ| B[ç›®éŒ„æƒæ]
    B -->|æ‰¾åˆ°| C[Parquetæª”æ¡ˆ]
    C -->|è®€å–| D[DataFrame]
    D -->|è§£æ| E[åƒæ•¸çµ„åˆ]
    D -->|æå–| F[ç¸¾æ•ˆæŒ‡æ¨™]
    D -->|æå–| G[æ¬Šç›Šæ›²ç·š]
    E -->|è¿”å›| H[æ•¸æ“šå­—å…¸]
    F -->|è¿”å›| H
    G -->|è¿”å›| H
```

ã€ç¶­è­·èˆ‡æ“´å……é‡é»ã€‘
------------------------------------------------------------
- æ–°å¢æ•¸æ“šæ ¼å¼ã€åƒæ•¸çµæ§‹æ™‚ï¼Œè«‹åŒæ­¥æ›´æ–°é ‚éƒ¨è¨»è§£èˆ‡å°æ‡‰æ¨¡çµ„
- è‹¥ parquet æª”æ¡ˆæ ¼å¼æœ‰è®Šå‹•ï¼Œéœ€åŒæ­¥æ›´æ–°è§£æé‚è¼¯
- æ–°å¢/ä¿®æ”¹æ•¸æ“šæ ¼å¼ã€åƒæ•¸çµæ§‹æ™‚ï¼Œå‹™å¿…åŒæ­¥æ›´æ–°æœ¬æª”æ¡ˆèˆ‡æ‰€æœ‰ä¾è³´æ¨¡çµ„
- æª”æ¡ˆè®€å–å’Œè§£æé‚è¼¯éœ€è¦ç‰¹åˆ¥æ³¨æ„éŒ¯èª¤è™•ç†

ã€å¸¸è¦‹æ˜“éŒ¯é»ã€‘
------------------------------------------------------------
- æª”æ¡ˆè·¯å¾‘éŒ¯èª¤æˆ–æª”æ¡ˆä¸å­˜åœ¨
- parquet æª”æ¡ˆæ ¼å¼ä¸ç¬¦åˆé æœŸ
- åƒæ•¸è§£æé‚è¼¯éŒ¯èª¤
- è¨˜æ†¶é«”ä½¿ç”¨éå¤§

ã€éŒ¯èª¤è™•ç†ã€‘
------------------------------------------------------------
- æª”æ¡ˆä¸å­˜åœ¨æ™‚æä¾›è©³ç´°éŒ¯èª¤è¨Šæ¯
- è§£æå¤±æ•—æ™‚æä¾›è¨ºæ–·å»ºè­°
- è¨˜æ†¶é«”ä¸è¶³æ™‚æä¾›å„ªåŒ–å»ºè­°

ã€ç¯„ä¾‹ã€‘
------------------------------------------------------------
- åŸºæœ¬ä½¿ç”¨ï¼šimporter = DataImporterPlotter("path/to/data")
- è¼‰å…¥æ•¸æ“šï¼šdata = importer.load_and_parse_data()

ã€èˆ‡å…¶ä»–æ¨¡çµ„çš„é—œè¯ã€‘
------------------------------------------------------------
- è¢« BasePlotter èª¿ç”¨
- ä¾è³´ metricstracker ç”¢ç”Ÿçš„ parquet æª”æ¡ˆæ ¼å¼
- è¼¸å‡ºæ•¸æ“šä¾› DashboardGenerator å’Œ CallbackHandler ä½¿ç”¨

ã€ç‰ˆæœ¬èˆ‡è®Šæ›´è¨˜éŒ„ã€‘
------------------------------------------------------------
- v1.0: åˆå§‹ç‰ˆæœ¬ï¼Œæ”¯æ´åŸºæœ¬æ•¸æ“šå°å…¥
- v1.1: æ–°å¢åƒæ•¸è§£æåŠŸèƒ½
- v1.2: æ–°å¢è¨˜æ†¶é«”å„ªåŒ–

ã€åƒè€ƒã€‘
------------------------------------------------------------
- è©³ç´°æµç¨‹è¦ç¯„å¦‚æœ‰è®Šå‹•ï¼Œè«‹åŒæ­¥æ›´æ–°æœ¬è¨»è§£èˆ‡ README
- å…¶ä»–æ¨¡çµ„å¦‚æœ‰ä¾è³´æœ¬æª”æ¡ˆçš„è¡Œç‚ºï¼Œè«‹æ–¼å°æ‡‰æ¨¡çµ„é ‚éƒ¨è¨»è§£æ¨™æ˜
- parquet æª”æ¡ˆæ ¼å¼è«‹åƒè€ƒ metricstracker æ¨¡çµ„
"""

import concurrent.futures
import glob
import json
import logging
import multiprocessing
import os
import warnings
from datetime import datetime
from typing import Any, Dict, List, Optional

import pandas as pd
import pyarrow.parquet as pq
from rich.console import Console
from rich.panel import Panel
from rich.text import Text

warnings.filterwarnings("ignore")

# æª¢æŸ¥ psutil æ˜¯å¦å¯ç”¨
PSUTIL_AVAILABLE = False
try:
    import psutil

    PSUTIL_AVAILABLE = True
except ImportError:
    pass


class DataImporterPlotter:
    """
    æ•¸æ“šå°å…¥å™¨

    è² è²¬è®€å–å’Œè§£æ metricstracker ç”¢ç”Ÿçš„ parquet æª”æ¡ˆï¼Œ
    æå–åƒæ•¸çµ„åˆã€ç¸¾æ•ˆæŒ‡æ¨™å’Œæ¬Šç›Šæ›²ç·šæ•¸æ“šã€‚
    """

    def __init__(self, data_path: str, logger: Optional[logging.Logger] = None):
        """
        åˆå§‹åŒ–æ•¸æ“šå°å…¥å™¨

        Args:
            data_path: metricstracker ç”¢ç”Ÿçš„ parquet æª”æ¡ˆç›®éŒ„è·¯å¾‘
            logger: æ—¥èªŒè¨˜éŒ„å™¨ï¼Œé è¨­ç‚º None
        """
        self.data_path = data_path
        self.logger = logger or logging.getLogger(__name__)
        self.logger.setLevel(logging.WARNING)
        # ä¸å†è‡ªå‹•åŠ  handlerï¼Œé¿å…é è¨­ log è¼¸å‡º
        # if not self.logger.hasHandlers():
        #     handler = logging.StreamHandler()
        #     formatter = logging.Formatter('[%(levelname)s] %(message)s')
        #     handler.setFormatter(formatter)
        #     self.logger.addHandler(handler)

        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        if not os.path.exists(self.data_path):
            raise FileNotFoundError(f"æ•¸æ“šç›®éŒ„ä¸å­˜åœ¨: {self.data_path}")

        # æ–°å¢ï¼šåˆå§‹åŒ–ç·©å­˜ç³»çµ±
        self.strategy_analysis_cache = {}
        self.parameter_index_cache = {}
        self.cache_stats = {"hits": 0, "misses": 0, "size": 0}

    def _get_memory_usage(self) -> Dict[str, float]:
        """ç²å–ç•¶å‰å…§å­˜ä½¿ç”¨æƒ…æ³"""
        if not PSUTIL_AVAILABLE:
            return {"available": 0, "used": 0, "percent": 0}

        try:
            memory = psutil.virtual_memory()
            return {
                "available": memory.available / 1024 / 1024 / 1024,  # GB
                "used": memory.used / 1024 / 1024 / 1024,  # GB
                "percent": memory.percent,
            }
        except Exception as e:
            self.logger.warning(f"ç²å–å…§å­˜ä½¿ç”¨å¤±æ•—: {e}")
            return {"available": 0, "used": 0, "percent": 0}

    def _log_memory_usage(self, stage: str):
        """è¨˜éŒ„å…§å­˜ä½¿ç”¨æƒ…æ³"""
        if PSUTIL_AVAILABLE:
            memory = self._get_memory_usage()
            self.logger.info(
                f"{stage} - å…§å­˜ä½¿ç”¨: {memory['used']:.2f}GB / {memory['percent']:.1f}%"
            )

    def scan_parquet_files(self) -> List[str]:
        """
        æƒæç›®éŒ„ä¸­çš„ parquet æª”æ¡ˆ

        Returns:
            List[str]: parquet æª”æ¡ˆè·¯å¾‘åˆ—è¡¨
        """
        try:
            pattern = os.path.join(self.data_path, "*.parquet")
            parquet_files = glob.glob(pattern)

            if not parquet_files:
                self.logger.warning(f"åœ¨ç›®éŒ„ {self.data_path} ä¸­æœªæ‰¾åˆ° parquet æª”æ¡ˆ")
                return []

            return sorted(parquet_files)

        except Exception as e:
            self.logger.error(f"æƒæ parquet æª”æ¡ˆå¤±æ•—: {e}")
            raise

    def parse_parameters_from_filename(self, filename: str) -> Dict[str, Any]:
        """
        å¾æª”æ¡ˆåç¨±è§£æåƒæ•¸çµ„åˆ

        Args:
            filename: æª”æ¡ˆåç¨±

        Returns:
            Dict[str, Any]: è§£æå‡ºçš„åƒæ•¸å­—å…¸
        """
        try:
            # ç§»é™¤è·¯å¾‘å’Œå‰¯æª”å
            basename = os.path.basename(filename)
            name_without_ext = os.path.splitext(basename)[0]

            # é è¨­åƒæ•¸çµæ§‹
            parameters = {"filename": basename, "reference_code": "", "parameters": {}}

            # å˜—è©¦è§£ææª”æ¡ˆåç¨±ä¸­çš„åƒæ•¸
            # æ ¼å¼ç¯„ä¾‹: 20250718_5ey6hl0q_metrics.parquet
            if "_metrics" in name_without_ext:
                parts = name_without_ext.split("_")
                if len(parts) >= 2:
                    parameters["reference_code"] = parts[1]

            return parameters

        except Exception as e:
            self.logger.warning(f"è§£ææª”æ¡ˆåç¨±åƒæ•¸å¤±æ•— {filename}: {e}")
            return {
                "filename": os.path.basename(filename),
                "reference_code": "",
                "parameters": {},
            }

    def extract_metrics_from_metadata(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        å¾ DataFrame çš„ metadata ä¸­æå–ç¸¾æ•ˆæŒ‡æ¨™

        Args:
            df: åŒ…å« metadata çš„ DataFrame

        Returns:
            Dict[str, Any]: ç¸¾æ•ˆæŒ‡æ¨™å­—å…¸
        """
        try:
            metrics = {}

            # æª¢æŸ¥æ˜¯å¦æœ‰ metadata
            if hasattr(df, "metadata") and df.metadata:
                # å˜—è©¦è§£æ strategy_metrics1
                if "strategy_metrics1" in df.metadata:
                    try:
                        strategy_metrics = json.loads(df.metadata["strategy_metrics1"])
                        metrics.update(strategy_metrics)
                    except (json.JSONDecodeError, TypeError) as e:
                        self.logger.warning(f"è§£æ strategy_metrics1 å¤±æ•—: {e}")

                # å˜—è©¦è§£æ bah_metrics1
                if "bah_metrics1" in df.metadata:
                    try:
                        bah_metrics = json.loads(df.metadata["bah_metrics1"])
                        metrics.update({f"bah_{k}": v for k, v in bah_metrics.items()})
                    except (json.JSONDecodeError, TypeError) as e:
                        self.logger.warning(f"è§£æ bah_metrics1 å¤±æ•—: {e}")

            return metrics

        except Exception as e:
            self.logger.warning(f"æå–ç¸¾æ•ˆæŒ‡æ¨™å¤±æ•—: {e}")
            return {}

    def extract_equity_curve_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æå–æ¬Šç›Šæ›²ç·šæ•¸æ“š

        Args:
            df: åŸå§‹ DataFrame

        Returns:
            pd.DataFrame: æ¬Šç›Šæ›²ç·šæ•¸æ“š
        """
        try:
            # ç¢ºä¿å¿…è¦çš„æ¬„ä½å­˜åœ¨
            required_columns = ["Time", "Equity_value"]
            missing_columns = [col for col in required_columns if col not in df.columns]

            if missing_columns:
                self.logger.warning(f"ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_columns}")
                return pd.DataFrame()

            # æå–æ¬Šç›Šæ›²ç·šç›¸é—œæ¬„ä½
            equity_columns = ["Time", "Equity_value", "Change"]
            available_columns = [col for col in equity_columns if col in df.columns]

            equity_data = df[available_columns].copy()

            # ç¢ºä¿ Time æ¬„ä½ç‚º datetime æ ¼å¼
            if "Time" in equity_data.columns:
                equity_data["Time"] = pd.to_datetime(equity_data["Time"])
                equity_data = equity_data.sort_values("Time")

            return equity_data

        except Exception as e:
            self.logger.warning(f"æå–æ¬Šç›Šæ›²ç·šæ•¸æ“šå¤±æ•—: {e}")
            return pd.DataFrame()

    def _load_single_parquet_file_optimized(
        self, file_path: str
    ) -> List[Dict[str, Any]]:
        """
        å„ªåŒ–çš„å–®å€‹parquetæª”æ¡ˆè¼‰å…¥é‚è¼¯ï¼ˆæ‰¹é‡è™•ç†ç‰ˆæœ¬ï¼‰

        Args:
            file_path: parquetæª”æ¡ˆè·¯å¾‘

        Returns:
            List[Dict[str, Any]]: åŒ…å«æ•¸æ“šå’Œå…ƒä¿¡æ¯çš„å­—å…¸åˆ—è¡¨
        """
        file_start_time = datetime.now()
        filename = os.path.basename(file_path)

        try:
            # æ­¥é©Ÿ1: è®€å–parquetæª”æ¡ˆ
            step1_start = datetime.now()
            table = pq.read_table(file_path)
            (datetime.now() - step1_start).total_seconds()

            # æ­¥é©Ÿ2: é¸æ“‡å¿…è¦åˆ—
            step2_start = datetime.now()
            required_columns = ["Time", "Equity_value", "BAH_Equity", "Backtest_id"]
            available_columns = [
                col for col in required_columns if col in table.column_names
            ]
            table = table.select(available_columns)
            (datetime.now() - step2_start).total_seconds()

            # æ­¥é©Ÿ3: è½‰æ›ç‚ºpandas
            step3_start = datetime.now()
            df = table.to_pandas()
            (datetime.now() - step3_start).total_seconds()

            # æ­¥é©Ÿ4: æå–metadataï¼ˆå„ªå…ˆå¾ JSON æª”æ¡ˆè®€å–ï¼‰
            step4_start = datetime.now()
            batch_metadata = []

            # å˜—è©¦å¾ JSON æª”æ¡ˆè®€å– metadataï¼ˆæ–°æ ¼å¼ï¼‰
            metadata_json_path = file_path.replace("_metrics.parquet", "_metadata.json")
            if os.path.exists(metadata_json_path):
                try:
                    with open(metadata_json_path, "r", encoding="utf-8") as f:
                        batch_metadata = json.load(f)
                except Exception as e:
                    self.logger.warning(f"ç„¡æ³•è®€å– JSON metadata: {e}")

            # å¦‚æœ JSON æª”æ¡ˆä¸å­˜åœ¨ï¼Œå˜—è©¦å¾ Parquet metadata è®€å–ï¼ˆå‘å¾Œç›¸å®¹ï¼‰
            if not batch_metadata:
                meta = table.schema.metadata or {}
                if b"batch_metadata" in meta:
                    try:
                        batch_metadata = json.loads(meta[b"batch_metadata"].decode())
                    except Exception as e:
                        self.logger.warning(f"ç„¡æ³•è®€å– Parquet metadata: {e}")

            (datetime.now() - step4_start).total_seconds()

            # æ­¥é©Ÿ5: æ‰¹é‡è™•ç†æ•¸æ“šï¼ˆå„ªåŒ–ç‰ˆæœ¬ï¼‰
            datetime.now()

            # æ‰¹é‡è™•ç†ï¼šä¸€æ¬¡æ€§åˆ†çµ„æ‰€æœ‰æ•¸æ“š
            grouped_data = {}
            for backtest_id, group in df.groupby("Backtest_id"):
                grouped_data[backtest_id] = {
                    "equity_curve": (
                        group[["Time", "Equity_value"]]
                        if "Equity_value" in group.columns
                        else None
                    ),
                    "bah_curve": (
                        group[["Time", "BAH_Equity"]]
                        if "BAH_Equity" in group.columns
                        else None
                    ),
                }

            # æ‰¹é‡å‰µå»ºçµæœ
            results = []
            for i, meta_item in enumerate(batch_metadata):
                backtest_id = meta_item.get("Backtest_id")
                if backtest_id is not None and backtest_id in grouped_data:
                    group_data = grouped_data[backtest_id]
                    results.append(
                        {
                            "Backtest_id": backtest_id,
                            "metrics": meta_item,
                            "equity_curve": group_data["equity_curve"],
                            "bah_curve": group_data["bah_curve"],
                            "file_path": file_path,
                        }
                    )
                else:
                    # å¦‚æœæ‰¾ä¸åˆ°å°æ‡‰çš„backtest_idï¼Œå‰µå»ºç©ºçš„çµæœ
                    results.append(
                        {
                            "Backtest_id": backtest_id,
                            "metrics": meta_item,
                            "equity_curve": None,
                            "bah_curve": None,
                            "file_path": file_path,
                        }
                    )

            # ç¸½è¨ˆæ™‚é–“
            (datetime.now() - file_start_time).total_seconds()

            return results

        except Exception as e:
            (datetime.now() - file_start_time).total_seconds()
            self.logger.error(f"å„ªåŒ–è¼‰å…¥æª”æ¡ˆå¤±æ•— {filename}: {e}")
            return []

    def load_parquet_file(self, file_path: str) -> List[Dict[str, Any]]:
        """
        è¼‰å…¥å–®å€‹ parquet æª”æ¡ˆ

        Args:
            file_path: parquet æª”æ¡ˆè·¯å¾‘

        Returns:
            List[Dict[str, Any]]: åŒ…å«æ•¸æ“šå’Œå…ƒä¿¡æ¯çš„å­—å…¸åˆ—è¡¨
        """
        try:
            self.logger.info(f"è¼‰å…¥æª”æ¡ˆ: {file_path}")

            # è®€å– parquet æª”æ¡ˆ
            df = pd.read_parquet(file_path)
            table = pq.read_table(file_path)
            batch_metadata = []

            # å˜—è©¦å¾ JSON æª”æ¡ˆè®€å– metadataï¼ˆæ–°æ ¼å¼ï¼‰
            metadata_json_path = file_path.replace("_metrics.parquet", "_metadata.json")
            if os.path.exists(metadata_json_path):
                try:
                    with open(metadata_json_path, "r", encoding="utf-8") as f:
                        batch_metadata = json.load(f)
                except Exception as e:
                    self.logger.warning(f"ç„¡æ³•è®€å– JSON metadata: {e}")

            # å¦‚æœ JSON æª”æ¡ˆä¸å­˜åœ¨ï¼Œå˜—è©¦å¾ Parquet metadata è®€å–ï¼ˆå‘å¾Œç›¸å®¹ï¼‰
            if not batch_metadata:
                meta = table.schema.metadata or {}
                if b"batch_metadata" in meta:
                    try:
                        batch_metadata = json.loads(meta[b"batch_metadata"].decode())
                    except Exception as e:
                        self.logger.warning(f"ç„¡æ³•è®€å– Parquet metadata: {e}")

                if not batch_metadata:
                    self.logger.warning(f"æ‰¾ä¸åˆ° batch_metadata: {file_path}")

            results = []
            for meta_item in batch_metadata:
                backtest_id = meta_item.get("Backtest_id")
                if backtest_id is not None and "Backtest_id" in df.columns:
                    df_bt = df[df["Backtest_id"] == backtest_id]
                else:
                    df_bt = df
                equity_curve = (
                    df_bt[["Time", "Equity_value"]]
                    if "Equity_value" in df_bt.columns
                    else None
                )
                bah_curve = (
                    df_bt[["Time", "BAH_Equity"]]
                    if "BAH_Equity" in df_bt.columns
                    else None
                )
                results.append(
                    {
                        "file_path": file_path,
                        "Backtest_id": backtest_id,
                        "metrics": meta_item,
                        "equity_curve": equity_curve,
                        "bah_curve": bah_curve,
                        "df": df_bt,
                    }
                )
                self.logger.debug(f"Backtest_id={backtest_id} metrics={meta_item}")
                self.logger.debug(
                    f"equity_curve len={len(equity_curve) if equity_curve is not None else 'None'}"
                )
                self.logger.debug(
                    f"bah_curve len={len(bah_curve) if bah_curve is not None else 'None'}"
                )
            return results

        except Exception as e:
            self.logger.error(f"è¼‰å…¥æª”æ¡ˆå¤±æ•— {file_path}: {e}")
            raise

    def load_parquet_files_parallel(
        self, file_paths: List[str]
    ) -> List[Dict[str, Any]]:
        """
        ä¸¦è¡Œè¼‰å…¥å¤šå€‹ parquet æª”æ¡ˆ

        Args:
            file_paths: parquet æª”æ¡ˆè·¯å¾‘åˆ—è¡¨

        Returns:
            List[Dict[str, Any]]: åŒ…å«æ•¸æ“šå’Œå…ƒä¿¡æ¯çš„å­—å…¸åˆ—è¡¨
        """
        try:
            self.logger.info(f"é–‹å§‹ä¸¦è¡Œè¼‰å…¥ {len(file_paths)} å€‹æª”æ¡ˆ")

            # ä½¿ç”¨é€²ç¨‹æ± ä¸¦è¡Œè®€å–
            max_workers = min(multiprocessing.cpu_count(), len(file_paths))
            self.logger.info(f"ä½¿ç”¨ {max_workers} å€‹é€²ç¨‹ä¸¦è¡Œè¼‰å…¥")

            with concurrent.futures.ProcessPoolExecutor(
                max_workers=max_workers
            ) as executor:
                # ä¸¦è¡Œæäº¤æ‰€æœ‰æª”æ¡ˆè¼‰å…¥ä»»å‹™
                future_to_file = {
                    executor.submit(
                        self._load_single_parquet_file_optimized, file_path
                    ): file_path
                    for file_path in file_paths
                }

                results = []
                completed_count = 0
                failed_count = 0

                # è™•ç†å®Œæˆçš„ä»»å‹™
                for future in concurrent.futures.as_completed(future_to_file):
                    file_path = future_to_file[future]
                    completed_count += 1

                    try:
                        file_data = future.result()
                        results.extend(file_data)

                        # é¡¯ç¤ºé€²åº¦
                        if completed_count % 5 == 0 or completed_count == len(
                            file_paths
                        ):
                            self.logger.info(
                                f"å·²è¼‰å…¥ {completed_count}/{len(file_paths)} å€‹æª”æ¡ˆ"
                            )

                    except Exception as e:
                        failed_count += 1
                        self.logger.error(f"ä¸¦è¡Œè™•ç†æª”æ¡ˆå¤±æ•— {file_path}: {e}")
                        continue

            self.logger.info(f"ä¸¦è¡Œè¼‰å…¥å®Œæˆï¼Œå…±è™•ç† {len(results)} å€‹æ•¸æ“šé …")
            return results

        except Exception as e:
            self.logger.error(f"ä¸¦è¡Œè¼‰å…¥å¤±æ•—: {e}")
            # å¦‚æœä¸¦è¡Œè¼‰å…¥å¤±æ•—ï¼Œå›é€€åˆ°ä¸²è¡Œè¼‰å…¥
            self.logger.warning("å›é€€åˆ°ä¸²è¡Œè¼‰å…¥æ¨¡å¼")
            return self._fallback_serial_load(file_paths)

    def _fallback_serial_load(self, file_paths: List[str]) -> List[Dict[str, Any]]:
        """
        ä¸²è¡Œè¼‰å…¥å›é€€æ–¹æ³•

        Args:
            file_paths: parquetæª”æ¡ˆè·¯å¾‘åˆ—è¡¨

        Returns:
            List[Dict[str, Any]]: åŒ…å«æ•¸æ“šå’Œå…ƒä¿¡æ¯çš„å­—å…¸åˆ—è¡¨
        """
        self.logger.info("ä½¿ç”¨ä¸²è¡Œè¼‰å…¥å›é€€æ–¹æ³•")
        results = []
        for file_path in file_paths:
            try:
                file_data = self._load_single_parquet_file_optimized(file_path)
                results.extend(file_data)
            except Exception as e:
                self.logger.error(f"ä¸²è¡Œè¼‰å…¥æª”æ¡ˆå¤±æ•— {file_path}: {e}")
                continue
        return results

    def load_and_parse_data(self) -> Dict[str, Any]:
        """
        è¼‰å…¥ä¸¦è§£ææ‰€æœ‰é¸å®šçš„ parquet æª”æ¡ˆï¼Œä¸¦åˆä½µæ‰€æœ‰ Backtest_id è³‡æ–™
        """
        start_time = datetime.now()
        self.logger.info("é–‹å§‹è¼‰å…¥å’Œè§£ææ•¸æ“š")

        try:
            # æƒææª”æ¡ˆ
            scan_start = datetime.now()
            self._log_memory_usage("æƒææª”æ¡ˆé–‹å§‹")
            parquet_files = self.scan_parquet_files()
            (datetime.now() - scan_start).total_seconds()
            self._log_memory_usage("æƒææª”æ¡ˆå®Œæˆ")

            if not parquet_files:
                raise FileNotFoundError("æœªæ‰¾åˆ°ä»»ä½• parquet æª”æ¡ˆ")

            # äº’å‹•å¼é¸å–®
            console = Console()
            # æ­¥é©Ÿèªªæ˜æ¡†
            step_content = (
                "ğŸŸ¢ é¸æ“‡è¦è¼‰å…¥çš„æª”æ¡ˆ\n"
                "ğŸ”´ ç”Ÿæˆå¯è¦–åŒ–ä»‹é¢[è‡ªå‹•]\n"
                "\n"
                "[bold #dbac30]èªªæ˜[/bold #dbac30]\n"
                "æ­¤æ­¥é©Ÿç”¨æ–¼é¸æ“‡è¦è¼‰å…¥çš„ parquet æª”æ¡ˆï¼Œæ”¯æ´å¤šæª”æ¡ˆåŒæ™‚è¼‰å…¥ã€‚\n"
                "æª”æ¡ˆåŒ…å«å›æ¸¬çµæœçš„ç¸¾æ•ˆæŒ‡æ¨™å’Œæ¬Šç›Šæ›²ç·šæ•¸æ“šã€‚\n\n"
                "[bold #dbac30]æª”æ¡ˆé¸æ“‡æ ¼å¼ï¼š[/bold #dbac30]\n"
                "â€¢ å–®ä¸€æª”æ¡ˆï¼šè¼¸å…¥æ•¸å­—ï¼ˆå¦‚ 1ï¼‰\n"
                "â€¢ å¤šæª”æ¡ˆï¼šç”¨é€—è™Ÿåˆ†éš”ï¼ˆå¦‚ 1,2,3ï¼‰\n"
                "â€¢ å…¨éƒ¨æª”æ¡ˆï¼šç›´æ¥æŒ‰ Enter\n\n"
                "[bold #dbac30]å¯é¸æ“‡çš„ parquet æª”æ¡ˆï¼š[/bold #dbac30]"
            )

            # æº–å‚™æª”æ¡ˆåˆ—è¡¨
            file_list = ""
            for i, f in enumerate(parquet_files, 1):
                file_list += (
                    f"  [bold #dbac30]{i}.[/bold #dbac30] {os.path.basename(f)}\n"
                )

            # çµ„åˆå®Œæ•´å…§å®¹ä¸¦ç”¨ Group é¡¯ç¤º
            complete_content = step_content + "\n" + file_list
            console.print(
                Panel(
                    complete_content,
                    title=Text("ğŸ‘ï¸ å¯è¦–åŒ– Plotter æ­¥é©Ÿï¼šæ•¸æ“šé¸æ“‡", style="bold #dbac30"),
                    border_style="#dbac30",
                )
            )

            # ç”¨æˆ¶è¼¸å…¥æç¤ºï¼ˆé‡‘è‰²+BOLDæ ¼å¼ï¼‰
            console.print("[bold #dbac30]è¼¸å…¥å¯è¦–åŒ–æª”æ¡ˆè™Ÿç¢¼ï¼š[/bold #dbac30]")
            file_input = input().strip() or "all"
            if not file_input:  # å¦‚æœè¼¸å…¥ç‚ºç©ºï¼Œè¼‰å…¥å…¨éƒ¨æª”æ¡ˆ
                selected_files = parquet_files
            else:
                try:
                    # è§£æç”¨æˆ¶è¼¸å…¥çš„æª”æ¡ˆç·¨è™Ÿ
                    file_indices = [int(x.strip()) for x in file_input.split(",")]
                    selected_files = [
                        parquet_files[i - 1]
                        for i in file_indices
                        if 1 <= i <= len(parquet_files)
                    ]
                    if not selected_files:
                        console.print(
                            Panel(
                                "âŒ æ²’æœ‰é¸æ“‡æœ‰æ•ˆçš„æª”æ¡ˆï¼Œé è¨­è¼‰å…¥å…¨éƒ¨æª”æ¡ˆã€‚",
                                title=Text("âš ï¸ è­¦å‘Š", style="bold #8f1511"),
                                border_style="#8f1511",
                            )
                        )
                        selected_files = parquet_files
                except (ValueError, IndexError):
                    console.print(
                        Panel(
                            "ğŸ”” å·²è‡ªå‹•è¼‰å…¥å…¨éƒ¨æª”æ¡ˆã€‚",
                            title=Text("ğŸ‘ï¸ å¯è¦–åŒ– Plotter", style="bold #8f1511"),
                            border_style="#dbac30",
                        )
                    )
                    selected_files = parquet_files

            # è¼‰å…¥æ‰€æœ‰é¸å®šæª”æ¡ˆ
            load_start = datetime.now()
            self._log_memory_usage("æª”æ¡ˆè¼‰å…¥é–‹å§‹")

            all_backtest_ids = []
            all_metrics = {}
            all_equity_curves = {}
            all_bah_curves = {}
            all_file_paths = {}
            all_parameters = []

            # ä½¿ç”¨ä¸¦è¡Œè¼‰å…¥æ›¿ä»£ä¸²è¡Œè¼‰å…¥
            try:
                self.logger.info("ä½¿ç”¨ä¸¦è¡Œè¼‰å…¥æ¨¡å¼")
                all_file_data = self.load_parquet_files_parallel(selected_files)

                for item in all_file_data:
                    backtest_id = item["Backtest_id"]
                    if backtest_id is not None:
                        all_backtest_ids.append(backtest_id)
                        all_parameters.append(item["metrics"])
                        all_metrics[backtest_id] = item["metrics"]
                        all_equity_curves[backtest_id] = item["equity_curve"]
                        all_bah_curves[backtest_id] = item["bah_curve"]
                        all_file_paths[backtest_id] = item["file_path"]

            except Exception as e:
                self.logger.warning(f"ä¸¦è¡Œè¼‰å…¥å¤±æ•—ï¼Œå›é€€åˆ°ä¸²è¡Œè¼‰å…¥: {e}")
                # å›é€€åˆ°åŸæœ‰çš„ä¸²è¡Œè¼‰å…¥æ–¹å¼
                for file_path in selected_files:
                    try:
                        file_data = self._load_single_parquet_file_optimized(file_path)
                        for item in file_data:
                            backtest_id = item["Backtest_id"]
                            if backtest_id is not None:
                                all_backtest_ids.append(backtest_id)
                                all_parameters.append(item["metrics"])
                                all_metrics[backtest_id] = item["metrics"]
                                all_equity_curves[backtest_id] = item["equity_curve"]
                                all_bah_curves[backtest_id] = item["bah_curve"]
                                all_file_paths[backtest_id] = item["file_path"]
                    except Exception as e:
                        self.logger.error(f"è™•ç†æª”æ¡ˆå¤±æ•— {file_path}: {e}")
                        continue

            (datetime.now() - load_start).total_seconds()
            self._log_memory_usage("æª”æ¡ˆè¼‰å…¥å®Œæˆ")

            if not all_parameters:
                raise ValueError("æ²’æœ‰æˆåŠŸè¼‰å…¥ä»»ä½•æª”æ¡ˆæˆ–æ‰¾åˆ° Backtest_id")

            # è­˜åˆ¥ç­–ç•¥åˆ†çµ„
            strategy_start = datetime.now()
            self._log_memory_usage("ç­–ç•¥åˆ†çµ„é–‹å§‹")
            strategy_groups = DataImporterPlotter.identify_strategy_groups(
                all_parameters
            )
            (datetime.now() - strategy_start).total_seconds()
            self._log_memory_usage("ç­–ç•¥åˆ†çµ„å®Œæˆ")

            # è¨˜éŒ„æ€§èƒ½çµ±è¨ˆ
            end_time = datetime.now()
            total_time = (end_time - start_time).total_seconds()

            self.logger.info(f"æ•¸æ“šè¼‰å…¥å®Œæˆçµ±è¨ˆ:")
            self.logger.info(f"  - ç¸½æª”æ¡ˆæ•¸: {len(selected_files)}")
            self.logger.info(f"  - ç¸½ç­–ç•¥æ•¸: {len(all_parameters)}")
            self.logger.info(f"  - ç¸½è€—æ™‚: {total_time:.2f}ç§’")
            self.logger.info(f"  - å¹³å‡æ¯æª”æ¡ˆ: {total_time/len(selected_files):.3f}ç§’")

            # é¡¯ç¤ºç·©å­˜çµ±è¨ˆï¼ˆå¦‚æœå•Ÿç”¨äº†ç·©å­˜ï¼‰
            if hasattr(self, "cache_stats"):
                cache_stats = self.get_cache_stats()
                self.logger.info(f"ç·©å­˜çµ±è¨ˆ: å‘½ä¸­ç‡ {cache_stats['hit_rate']:.2%}")

            result = {
                "dataframes": all_metrics,
                "parameters": all_parameters,
                "metrics": all_metrics,
                "equity_curves": all_equity_curves,
                "bah_curves": all_bah_curves,
                "file_paths": all_file_paths,
                "backtest_ids": all_backtest_ids,  # æ–°å¢Backtest_idåˆ—è¡¨
                "strategy_groups": strategy_groups,  # æ–°å¢ç­–ç•¥åˆ†çµ„ä¿¡æ¯
                "total_files": len(selected_files),
                "loaded_at": datetime.now().isoformat(),
                "load_time_seconds": total_time,  # æ–°å¢è¼‰å…¥æ™‚é–“çµ±è¨ˆ
                "cache_stats": (
                    self.get_cache_stats() if hasattr(self, "cache_stats") else None
                ),  # æ–°å¢ç·©å­˜çµ±è¨ˆ
            }
            return result
        except Exception as e:
            self.logger.error(f"è¼‰å…¥å’Œè§£ææ•¸æ“šå¤±æ•—: {e}")
            raise

    def get_parameter_summary(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ç²å–åƒæ•¸æ‘˜è¦ä¿¡æ¯

        Args:
            data: è¼‰å…¥çš„æ•¸æ“šå­—å…¸

        Returns:
            Dict[str, Any]: åƒæ•¸æ‘˜è¦
        """
        try:
            parameters = data.get("parameters", {})

            # çµ±è¨ˆåƒæ•¸åˆ†å¸ƒ
            param_summary = {}
            for param_key, param_data in parameters.items():
                param_dict = param_data.get("parameters", {})
                for key, value in param_dict.items():
                    if key not in param_summary:
                        param_summary[key] = set()
                    param_summary[key].add(str(value))

            # è½‰æ›ç‚ºåˆ—è¡¨
            for key in param_summary:
                param_summary[key] = sorted(list(param_summary[key]))

            return {
                "total_combinations": len(parameters),
                "parameter_distribution": param_summary,
                "parameter_keys": list(param_summary.keys()),
            }

        except Exception as e:
            self.logger.warning(f"ç²å–åƒæ•¸æ‘˜è¦å¤±æ•—: {e}")
            return {}

    def filter_data_by_parameters(
        self, data: Dict[str, Any], filters: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        æ ¹æ“šåƒæ•¸ç¯©é¸æ•¸æ“š

        Args:
            data: åŸå§‹æ•¸æ“šå­—å…¸
            filters: ç¯©é¸æ¢ä»¶å­—å…¸

        Returns:
            Dict[str, Any]: ç¯©é¸å¾Œçš„æ•¸æ“šå­—å…¸
        """
        try:
            if not filters:
                return data

            parameters = data.get("parameters", {})
            filtered_keys = set(parameters.keys())

            # æ‡‰ç”¨ç¯©é¸æ¢ä»¶
            for param_name, param_values in filters.items():
                if not param_values:  # ç©ºå€¼è¡¨ç¤ºä¸éæ¿¾
                    continue

                matching_keys = set()
                for key, param_data in parameters.items():
                    param_dict = param_data.get("parameters", {})
                    if param_name in param_dict:
                        param_value = str(param_dict[param_name])
                        if param_value in param_values:
                            matching_keys.add(key)

                filtered_keys = filtered_keys.intersection(matching_keys)

            # æ§‹å»ºç¯©é¸å¾Œçš„æ•¸æ“š
            filtered_data = {}
            for key in filtered_keys:
                filtered_data[key] = data.get(key, {})

            return filtered_data

        except Exception as e:
            self.logger.warning(f"åƒæ•¸ç¯©é¸å¤±æ•—: {e}")
            return data

    @staticmethod
    def parse_all_parameters(parameters: list) -> dict:
        """
        å‹•æ…‹å±•é–‹æ‰€æœ‰ Entry_params/Exit_paramsï¼Œå›å‚³ {åƒæ•¸å: [æ‰€æœ‰å€¼]}ï¼ŒåŒä¸€åƒæ•¸åªåˆ—ä¸€æ¬¡ã€‚
        """
        from .utils.ParameterParser_utils_plotter import ParameterParser

        return ParameterParser.parse_all_parameters(parameters)

    @staticmethod
    def parse_entry_exit_parameters(parameters: list):
        """
        åˆ†åˆ¥å±•é–‹ Entry_params/Exit_paramsï¼Œå›å‚³ (entry_param_values, exit_param_values)
        """
        from .utils.ParameterParser_utils_plotter import ParameterParser

        return ParameterParser.parse_entry_exit_parameters(parameters)

    @staticmethod
    def parse_indicator_param_structure(parameters: list):
        """
        çµ±è¨ˆæ‰€æœ‰ entry/exit ä¸‹ indicator_type åŠå…¶æ‰€æœ‰åƒæ•¸åèˆ‡å€¼ï¼š
        å›å‚³ {
            'entry': {indicator_type: {param: [å€¼]}},
            'exit': {indicator_type: {param: [å€¼]}}
        }
        """
        from .utils.ParameterParser_utils_plotter import ParameterParser

        return ParameterParser.parse_indicator_param_structure(parameters)

    @staticmethod
    def identify_strategy_groups(parameters: list) -> Dict[str, Any]:
        """
        è­˜åˆ¥ç­–ç•¥åˆ†çµ„ï¼ŒåŸºæ–¼ Entry_params å’Œ Exit_params çš„ indicator_type + strat_idx çµ„åˆ

        Args:
            parameters: åƒæ•¸åˆ—è¡¨

        Returns:
            Dict[str, Any]: ç­–ç•¥åˆ†çµ„ä¿¡æ¯
        """
        from .utils.ParameterParser_utils_plotter import ParameterParser

        return ParameterParser.identify_strategy_groups(parameters)

    @staticmethod
    def analyze_strategy_parameters(
        parameters: list, strategy_key: str
    ) -> Dict[str, Any]:
        """
        åˆ†æé¸ä¸­ç­–ç•¥çš„å¯è®Šåƒæ•¸ï¼Œç”¨æ–¼ç”Ÿæˆ2Dåƒæ•¸é«˜åŸåœ–è¡¨

        æ”¯æŒå¤šæŒ‡æ¨™ç­–ç•¥å’Œå‹•æ…‹åƒæ•¸è­˜åˆ¥

        Args:
            parameters: åƒæ•¸åˆ—è¡¨
            strategy_key: é¸ä¸­çš„ç­–ç•¥éµ

        Returns:
            Dict[str, Any]: åƒæ•¸åˆ†æçµæœ
        """
        from .utils.ParameterParser_utils_plotter import ParameterParser

        return ParameterParser.analyze_strategy_parameters(parameters, strategy_key)

    def get_cache_stats(self) -> Dict[str, Any]:
        """ç²å–ç·©å­˜çµ±è¨ˆä¿¡æ¯"""
        return {
            "hits": self.cache_stats["hits"],
            "misses": self.cache_stats["misses"],
            "size": self.cache_stats["size"],
            "hit_rate": (
                self.cache_stats["hits"]
                / (self.cache_stats["hits"] + self.cache_stats["misses"])
                if (self.cache_stats["hits"] + self.cache_stats["misses"]) > 0
                else 0
            ),
        }

    def get_strategy_analysis_cached(
        self, parameters: list, strategy_key: str
    ) -> Dict[str, Any]:
        """
        ç²å–ç­–ç•¥åˆ†æçµæœï¼ˆå¸¶ç·©å­˜ï¼‰

        Args:
            parameters: åƒæ•¸åˆ—è¡¨
            strategy_key: é¸ä¸­çš„ç­–ç•¥éµ

        Returns:
            Dict[str, Any]: åƒæ•¸åˆ†æçµæœ
        """
        # å‰µå»ºç·©å­˜éµ
        cache_key = f"analysis_{strategy_key}_{len(parameters)}"

        # æª¢æŸ¥ç·©å­˜
        if (
            hasattr(self, "strategy_analysis_cache")
            and cache_key in self.strategy_analysis_cache
        ):
            self.cache_stats["hits"] += 1
            self.logger.debug(f"ç·©å­˜å‘½ä¸­: {strategy_key}")
            return self.strategy_analysis_cache[cache_key]

        # ç·©å­˜æœªå‘½ä¸­ï¼ŒåŸ·è¡Œåˆ†æ
        self.cache_stats["misses"] += 1
        self.logger.debug(f"ç·©å­˜æœªå‘½ä¸­ï¼ŒåŸ·è¡Œåˆ†æ: {strategy_key}")

        # ä½¿ç”¨éœæ…‹æ–¹æ³•é€²è¡Œåˆ†æ
        analysis = self.analyze_strategy_parameters(parameters, strategy_key)

        # å­˜å…¥ç·©å­˜
        if hasattr(self, "strategy_analysis_cache"):
            self.strategy_analysis_cache[cache_key] = analysis
            self.cache_stats["size"] += 1
        else:
            self.logger.warning("ç·©å­˜ç³»çµ±æœªåˆå§‹åŒ–ï¼")

        # å¦‚æœç·©å­˜éå¤§ï¼Œæ¸…ç†èˆŠçš„ç·©å­˜é …
        if hasattr(self, "cache_stats") and self.cache_stats["size"] > 100:
            self._cleanup_cache()

        return analysis

    def _cleanup_cache(self):
        """æ¸…ç†ç·©å­˜ï¼Œä¿ç•™æœ€è¿‘ä½¿ç”¨çš„é …ç›®"""
        if len(self.strategy_analysis_cache) > 50:
            # ç°¡å–®çš„ç·©å­˜æ¸…ç†ï¼šåˆªé™¤ä¸€åŠçš„ç·©å­˜é …
            keys_to_remove = list(self.strategy_analysis_cache.keys())[:25]
            for key in keys_to_remove:
                del self.strategy_analysis_cache[key]
            self.cache_stats["size"] = len(self.strategy_analysis_cache)
            self.logger.debug(f"ç·©å­˜æ¸…ç†å®Œæˆï¼Œç•¶å‰å¤§å°: {self.cache_stats['size']}")
